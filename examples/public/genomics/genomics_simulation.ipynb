{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genomics example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will explore how importance scores from three different methods compare on simulated genomic data.\n",
    "\n",
    "The simulated data was as follows:\n",
    "- 1/4 sequences with 1-3 instances of a GATA_disc1 motif embedded (see http://compbio.mit.edu/encode-motifs/ for the PWM); these were labelled 1,0,0\n",
    "- 1/4 sequences with 1-3 instances of a TAL1_known1 motif embedded; these were labelled 0,1,0\n",
    "- 1/4 sequences with BOTH 1-3 instances of a GATA_disc1 motif AND 1-3 instances of a TAL1_known1 motif; these were labelled 1,1,1\n",
    "- 1/4 sequences with no motif\n",
    "\n",
    "Scores for all three tasks for sequences that contain both TAL1_known1 and GATA_disc1 motifs are analyzed in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain data and keras model\n",
    "\n",
    "We will download genomic data and model\n",
    "\n",
    "### Download the data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!./grab_model_and_data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and one-hot encode the data\n",
    "\n",
    "The simdna package is needed for reading the data; install it if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import simdna\n",
    "except ImportError, e:\n",
    "    print(\"installing simdna package\")\n",
    "    !pip install -e \"git://github.com/kundajelab/simdna.git@0.4.0#egg=simdna\"\n",
    "    print(\"\\n******************************************************************************\")\n",
    "    print(\"RESTART THE JUPYTER KERNEL TO PICK UP ON THE INSTALLATION!!!\")\n",
    "    print(\"******************************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simdna.synthetic as synthetic\n",
    "reload(synthetic)\n",
    "reload(synthetic.core)\n",
    "import gzip\n",
    "data_filename = \"sequences.simdata.gz\"\n",
    "\n",
    "#read in the data in the testing set\n",
    "test_ids_fh = gzip.open(\"test.txt.gz\",\"rb\")\n",
    "ids_to_load = [x.rstrip(\"\\n\") for x in test_ids_fh]\n",
    "data = synthetic.read_simdata_file(data_filename, ids_to_load=ids_to_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#this model was trained on data one-hot encoded as a 2d image, with the row-axis being the axis\n",
    "#for one-hot encoding.\n",
    "def one_hot_encode_along_row_axis(sequence):\n",
    "    #theano dim ordering, uses row axis for one-hot\n",
    "    to_return = np.zeros((1,4,len(sequence)), dtype=np.int8)\n",
    "    seq_to_one_hot_fill_in_array(zeros_array=to_return[0],\n",
    "                                 sequence=sequence, one_hot_axis=0)\n",
    "    return to_return\n",
    "\n",
    "def seq_to_one_hot_fill_in_array(zeros_array, sequence, one_hot_axis):\n",
    "    assert one_hot_axis==0 or one_hot_axis==1\n",
    "    if (one_hot_axis==0):\n",
    "        assert zeros_array.shape[1] == len(sequence)\n",
    "    elif (one_hot_axis==1): \n",
    "        assert zeros_array.shape[0] == len(sequence)\n",
    "    #zeros_array should be an array of dim 4xlen(sequence), filled with zeros.\n",
    "    #will mutate zeros_array\n",
    "    for (i,char) in enumerate(sequence):\n",
    "        if (char==\"A\" or char==\"a\"):\n",
    "            char_idx = 0\n",
    "        elif (char==\"C\" or char==\"c\"):\n",
    "            char_idx = 1\n",
    "        elif (char==\"G\" or char==\"g\"):\n",
    "            char_idx = 2\n",
    "        elif (char==\"T\" or char==\"t\"):\n",
    "            char_idx = 3\n",
    "        elif (char==\"N\" or char==\"n\"):\n",
    "            continue #leave that pos as all 0's\n",
    "        else:\n",
    "            raise RuntimeError(\"Unsupported character: \"+str(char))\n",
    "        if (one_hot_axis==0):\n",
    "            zeros_array[char_idx,i] = 1\n",
    "        elif (one_hot_axis==1):\n",
    "            zeros_array[i,char_idx] = 1\n",
    "            \n",
    "onehot_data = np.array([one_hot_encode_along_row_axis(seq) for seq in data.sequences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplift\n",
    "import deeplift.conversion.keras_conversion as kc\n",
    "\n",
    "#load the keras model\n",
    "keras_model_weights = \"record_5_model_PQzyq_modelWeights.h5\"\n",
    "keras_model_json = \"record_5_model_PQzyq_modelJson.json\"\n",
    "\n",
    "keras_model = kc.load_keras_model(weights=keras_model_weights,\n",
    "                                  json=keras_model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the deeplift models\n",
    "\n",
    "### Model conversion\n",
    "\n",
    "Convert the keras models to deeplift models capable of computing importance scores using DeepLIFT (with 3 different variants: rescale on the conv layers and revealcancel on the fully-connected layers (the genomics default), rescale on all layers, and revealcancel on all layers), gradients and guided backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplift.blobs import NonlinearMxtsMode\n",
    "from collections import OrderedDict\n",
    "\n",
    "method_to_model = OrderedDict()\n",
    "for method_name, nonlinear_mxts_mode in [\n",
    "    #The genomics default = rescale on conv layers, revealcance on fully-connected\n",
    "    ('rescale_conv_revealcancel_fc', NonlinearMxtsMode.DeepLIFT_GenomicsDefault),\n",
    "    ('rescale_all_layers', NonlinearMxtsMode.Rescale),\n",
    "    ('revealcancel_all_layers', NonlinearMxtsMode.RevealCancel),\n",
    "    ('grad_times_inp', NonlinearMxtsMode.Gradient),\n",
    "    ('guided_backprop', NonlinearMxtsMode.GuidedBackprop)]:\n",
    "    method_to_model[method_name] = kc.convert_sequential_model(\n",
    "        model=keras_model,\n",
    "        nonlinear_mxts_mode=nonlinear_mxts_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks\n",
    "\n",
    "To ensure that the conversion happend correctly, ensure that the models give identical predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure predictions are the same as the original model\n",
    "from deeplift.util import compile_func\n",
    "model_to_test = method_to_model['rescale_conv_revealcancel_fc']\n",
    "deeplift_prediction_func = compile_func([model_to_test.get_layers()[0].get_activation_vars()],\n",
    "                                         model_to_test.get_layers()[-1].get_activation_vars())\n",
    "original_model_predictions = keras_model.predict(onehot_data, batch_size=200)\n",
    "converted_model_predictions = deeplift.util.run_function_in_batches(\n",
    "                                input_data_list=[onehot_data],\n",
    "                                func=deeplift_prediction_func,\n",
    "                                batch_size=200,\n",
    "                                progress_update=None)\n",
    "print(\"maximum difference in predictions:\",np.max(np.array(converted_model_predictions)-np.array(original_model_predictions)))\n",
    "assert np.max(np.array(converted_model_predictions)-np.array(original_model_predictions)) < 10**-5\n",
    "predictions = converted_model_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute importance scores\n",
    "\n",
    "### Compile various scoring functions\n",
    "\n",
    "Using the deeplift models, we obtain the functions capable of computing the importance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Compiling scoring functions\")\n",
    "method_to_scoring_func = OrderedDict()\n",
    "for method,model in method_to_model.items():\n",
    "    print(\"Compiling scoring function for: \"+method)\n",
    "    method_to_scoring_func[method] = model.get_target_contribs_func(find_scores_layer_idx=0,\n",
    "                                                                    target_layer_idx=-2)\n",
    "    \n",
    "#To get a function that just gives the gradients, we use the multipliers of the Gradient model\n",
    "gradient_func = method_to_model['grad_times_inp'].get_target_multipliers_func(find_scores_layer_idx=0,\n",
    "                                                                              target_layer_idx=-2)\n",
    "print(\"Compiling integrated gradients scoring functions\")\n",
    "integrated_gradients10_func = deeplift.util.get_integrated_gradients_function(\n",
    "    gradient_computation_function = gradient_func,\n",
    "    num_intervals=10)\n",
    "method_to_scoring_func['integrated_gradients10'] = integrated_gradients10_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call scoring functions on the data\n",
    "\n",
    "In the cell below, a reference representing 40\\% GC content is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = OrderedDict([('A', 0.3), ('C', 0.2), ('G', 0.2), ('T', 0.3)])\n",
    "\n",
    "from collections import OrderedDict\n",
    "method_to_task_to_scores = OrderedDict()\n",
    "for method_name, score_func in method_to_scoring_func.items():\n",
    "    print(\"on method\",method_name)\n",
    "    method_to_task_to_scores[method_name] = OrderedDict()\n",
    "    for task_idx in [0,1,2]:\n",
    "        scores = np.array(score_func(\n",
    "                    task_idx=task_idx,\n",
    "                    input_data_list=[onehot_data],\n",
    "                    input_references_list=[\n",
    "                     np.array([background['A'],\n",
    "                               background['C'],\n",
    "                               background['G'],\n",
    "                               background['T']])[None,None,:,None]],\n",
    "                    batch_size=200,\n",
    "                    progress_update=None))\n",
    "        assert scores.shape[2]==4\n",
    "        scores = np.squeeze(np.sum(scores, axis=2),axis=1)\n",
    "        method_to_task_to_scores[method_name][task_idx] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using multiple shuffled references\n",
    "\n",
    "As an alternative to using a flat reference based on GC content (which can sometimes produce artefacts), we propose averaging the scores produced using mutliple references which are produced by shuffling the original sequence. We find in practice that this can give more robust results. Not that in general, the optimal choice of reference is an area of active research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(deeplift.util)\n",
    "from deeplift.util import get_shuffle_seq_ref_function\n",
    "#from deeplift.util import randomly_shuffle_seq\n",
    "from deeplift.dinuc_shuffle import dinuc_shuffle #function to do a dinucleotide shuffle\n",
    "\n",
    "rescale_conv_revealcancel_fc_many_refs_func = get_shuffle_seq_ref_function(\n",
    "    #score_computation_function is the original function to compute scores\n",
    "    score_computation_function=method_to_scoring_func['rescale_conv_revealcancel_fc'],\n",
    "    #shuffle_func is the function that shuffles the sequence\n",
    "    #technically, given the background of this simulation, randomly_shuffle_seq\n",
    "    #makes more sense. However, on real data, a dinuc shuffle is advisable due to\n",
    "    #the strong bias against CG dinucleotides\n",
    "    shuffle_func=dinuc_shuffle,\n",
    "    one_hot_func=lambda x: np.array([one_hot_encode_along_row_axis(seq) for seq in x]))\n",
    "\n",
    "num_refs_per_seq=10 #number of references to generate per sequence\n",
    "method_to_task_to_scores['rescale_conv_revealcancel_fc_multiref_'+str(num_refs_per_seq)] = OrderedDict()\n",
    "for task_idx in [0,1,2]:\n",
    "    method_to_task_to_scores['rescale_conv_revealcancel_fc_multiref_'+str(num_refs_per_seq)][task_idx] =\\\n",
    "        np.squeeze(np.sum(rescale_conv_revealcancel_fc_many_refs_func(\n",
    "            task_idx=task_idx,\n",
    "            input_data_sequences=data.sequences,\n",
    "            num_refs_per_seq=num_refs_per_seq,\n",
    "            batch_size=200,\n",
    "            progress_update=1000,\n",
    "        ),axis=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize scores on individual sequences\n",
    "\n",
    "Visualize the scores at specific sequences. Cyan boxes indicate the ground-truth locations of the inserted TAL1_known1 motifs, red boxes indicate the ground-truth locations of the inserted GATA_disc1 motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize scores + ground-truth locations of motifs\n",
    "%matplotlib inline\n",
    "from deeplift.visualization import viz_sequence\n",
    "\n",
    "for task, idx in [(0,731), #illustrates failure of grad*inp, integrated grads, deeplift-rescale\n",
    "                  (1,197)  #illustrates non-specific firing of guided backprop\n",
    "                 ]:\n",
    "    print(\"Scores for task\",task,\"for example\",idx)\n",
    "    for method_name in [\n",
    "                        'grad_times_inp',\n",
    "                        'guided_backprop',\n",
    "                        'integrated_gradients10',\n",
    "                        'rescale_all_layers', 'revealcancel_all_layers',\n",
    "                        'rescale_conv_revealcancel_fc',\n",
    "                        'rescale_conv_revealcancel_fc_multiref_10'\n",
    "                        ]:\n",
    "        scores = method_to_task_to_scores[method_name][task]\n",
    "        scores_for_idx = scores[idx]\n",
    "        original_onehot = onehot_data[idx]\n",
    "        scores_for_idx = original_onehot*scores_for_idx[None,None,:]\n",
    "        print(method_name)\n",
    "        highlight = {'blue':[\n",
    "                (embedding.startPos, embedding.startPos+len(embedding.what))\n",
    "                for embedding in data.embeddings[idx] if 'GATA_disc1' in embedding.what.getDescription()],\n",
    "                'green':[\n",
    "                (embedding.startPos, embedding.startPos+len(embedding.what))\n",
    "                for embedding in data.embeddings[idx] if 'TAL1_known1' in embedding.what.getDescription()]}\n",
    "        viz_sequence.plot_weights(scores_for_idx, subticks_frequency=10, highlight=highlight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterplots of motif strength vs. importance scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the results on individual examples is one thing, but how can we get a sense of the performance across all examples? Fortunately, because we simulated the data, we know the true PWMs that the motif instances were sampled from. Thus, for every sequence, we can identify the top 5 matches to a given PWM and also investigate the total importance assigned to the positions underlying those matches. Depending on the task and the particular sequence, we should find that strong matches to certain PWMs are very relevant for some tasks and not relevant for others. The code for performing that analysis is given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the top motif matches and the strength of the scores underlying them\n",
    "\n",
    "Scores are computed for different tasks, and using different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import simdna\n",
    "from simdna import synthetic as sn\n",
    "%matplotlib inline\n",
    "\n",
    "#dictionary mapping the name of the motif to the strength of the top N matches per sequence\n",
    "motif_to_top_N_log_odds_scores = OrderedDict()\n",
    "#dictionary mapping the name of the motif to the incides of the top N matches per sequence\n",
    "motif_to_top_N_indices = OrderedDict()\n",
    "#dictionary mapping the name of the motif to a\n",
    "# [dictionary mapping the task to a\n",
    "#  [ dictionary mapping the method name to the total score atop the corresponding top N motif matches]]\n",
    "motif_to_task_to_method_to_corresponding_score = OrderedDict()\n",
    "\n",
    "for motif_name in ['GATA_disc1', 'TAL1_known1']:\n",
    "    #get the motif locations\n",
    "    #load a file representing the full set of ENCODE motifs\n",
    "    loaded_motifs = sn.LoadedEncodeMotifs(simdna.ENCODE_MOTIFS_PATH,\n",
    "                                        pseudocountProb=0.001)\n",
    "    #get the PWM\n",
    "    pwm_rows = loaded_motifs.getPwm(motif_name).getRows()\n",
    "    \n",
    "    #convert PWM to a log-odds matrix\n",
    "    log_odds_mat = np.log(pwm_rows) - np.log(np.array([background['A'], background['C'], background['G'], background['T']])[None,:])\n",
    "    #transpose it so ACGT is on the first axis\n",
    "    log_odds_mat = log_odds_mat.T\n",
    "    #compile a function that will do the cross-correlation with the log-odds matrix\n",
    "    cross_corr_func = deeplift.util.get_cross_corr_function(filters=np.array([log_odds_mat]))\n",
    "    #get the scores using the cross-correlation with the log-odds matrix\n",
    "    log_odds_scores = np.squeeze(cross_corr_func(onehot_data, batch_size=20))\n",
    "    #log-odds matrix was transposed, so now the length of the motif is the second dim\n",
    "    motif_size = log_odds_mat.shape[-1] \n",
    "    \n",
    "    #for reach region, retain top n non-overlapping log-odds scores (here n=5)\n",
    "    #the top n hits are selected greedily; every time a hit location is found, other\n",
    "    #hits within exclude_hits_within_window are ignored. This is to prevent overlapping\n",
    "    #hits from being returned.\n",
    "    #top_n_log_odds_scores returns the top n scores per region.\n",
    "    #top_n_indices returns the indices (location within the sequence; left edge) at which the hits were found\n",
    "    top_n_log_odds_scores, top_n_indices = deeplift.util.get_top_n_scores_per_region(\n",
    "        log_odds_scores, n=5, exclude_hits_within_window=int(motif_size/2))\n",
    "    \n",
    "    #store the hits and their locations in the dictionaries\n",
    "    motif_to_top_N_log_odds_scores[motif_name] = top_n_log_odds_scores\n",
    "    motif_to_top_N_indices[motif_name] = top_n_indices\n",
    "    \n",
    "    #now compile the values for the total score at those motif hits for different tasks, using different methods\n",
    "    task_to_method_to_corresponding_score = OrderedDict()\n",
    "    motif_to_task_to_method_to_corresponding_score[motif_name] = task_to_method_to_corresponding_score\n",
    "    \n",
    "    #get the scores for different tasks\n",
    "    for task_idx, task_description in [\n",
    "        (0, \"0 (both-tal-and-gata)\"),\n",
    "        (1, '1 (gata-only)'),\n",
    "        (2, '2 (tal-only)')]:\n",
    "        \n",
    "        task_to_method_to_corresponding_score[task_idx] = OrderedDict()\n",
    "    \n",
    "        #iterate over different methods\n",
    "        for method in method_to_task_to_scores:\n",
    "            \n",
    "            scores = method_to_task_to_scores[method][task_idx]\n",
    "            #the smoothen function averages scores within a window of size motif_size\n",
    "            #same_size_return=False just means we don't want to bother with padding the arrays to make\n",
    "            #sure that the returned array is the same size as the supplied array\n",
    "            #when this smoothen function is called, the value at index i represents the average\n",
    "            #scores for the indices i:i+motif_size\n",
    "            smoothen_function = deeplift.util.get_smoothen_function(motif_size, same_size_return=False)\n",
    "            #we want the total scores on top of the hits, not the average scores, so we scale up the results of calling\n",
    "            #the smoothen function by motif_size\n",
    "            sum_scores = np.array(smoothen_function(scores, batch_size=20))*motif_size #scale it up            \n",
    "            \n",
    "            #Finally, we just index into sum_scores according to the locations of the top n motif\n",
    "            #hits to retrieve the total score atop those motif hits\n",
    "            corresponding_sum_scores = []\n",
    "            for scores_this_region, indices in zip(sum_scores, top_n_indices):\n",
    "                corresponding_sum_scores.append([scores_this_region[idx] for idx in indices])\n",
    "            \n",
    "            task_to_method_to_corresponding_score[task_idx][method] = np.array(corresponding_sum_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the scatterplots\n",
    "\n",
    "Note that there may be minor differences between these figures and the one in the paper because the figures in the paper were generated on the validation set, while here for simplicity we generate the figures on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "#compile a list of the methods to make the plots for\n",
    "methods_and_names = [\n",
    "        ('guided_backprop', 'Guided\\nBackprop*inp'),\n",
    "        ('grad_times_inp', 'gradient*\\ninput'),\n",
    "        ('integrated_gradients10', 'integrated\\ngradients-10'),\n",
    "        ('rescale_all_layers', 'DeepLIFT-\\nRescale'),\n",
    "        ('revealcancel_all_layers', 'DeepLIFT-\\nRevealCancel'),\n",
    "        ('rescale_conv_revealcancel_fc', 'DeepLIFT-\\nfc-RC-conv-RS'),\n",
    "        ('rescale_conv_revealcancel_fc_multiref_10', 'DeepLIFT-\\nfc-RC-conv-RS\\nMultiref-10')\n",
    "    ]\n",
    "\n",
    "#function to randomly shuffle the input arrays (correspondingly)\n",
    "def shuffle_arrays(arrs):\n",
    "    if len(arrs) == 0:\n",
    "        raise ValueError(\"should supply at least one input array\")\n",
    "    len_of_arrs = len(arrs[0])\n",
    "    #sanity check that all lengths are equal\n",
    "    for arr in arrs:\n",
    "        if (len(arr) != len_of_arrs):\n",
    "            raise ValueError(\"First supplied array had length \"\n",
    "                             +str(len_of_arrs)\n",
    "                             +\" but a subsequent array had length \"\n",
    "                             +str(len(arr)))\n",
    "    for i in xrange(0,len_of_arrs):\n",
    "        #randomly select index:\n",
    "        chosen_index = random.randint(i,len_of_arrs-1)\n",
    "        for arr in arrs:\n",
    "            #swap\n",
    "            val_at_index = arr[chosen_index]\n",
    "            arr[chosen_index] = arr[i]\n",
    "            arr[i] = val_at_index\n",
    "    return arrs\n",
    "\n",
    "#iterate over the two motifs\n",
    "for motif_name in ['GATA_disc1', 'TAL1_known1']:\n",
    "    #method_to_task_0_max_y is used so that the scale used for task 0 is\n",
    "    #also applied to the task for which the motif is irrelevant\n",
    "    #(irrelevant = task 2 for GATA and task 1 for TAL)\n",
    "    #This makes it easier to tell, by eye, whether the method has a \n",
    "    #tendency to produce false positives.\n",
    "    method_to_task_0_max_y = OrderedDict()\n",
    "    #frac_task_0_red_le_zero is the fraction of high-scoring motifs\n",
    "    #in regions that had both an embedded TAL and an embedded GATA that\n",
    "    #were given importance of < 0 (false negatives)\n",
    "    frac_task_0_red_le_zero = OrderedDict()\n",
    "    \n",
    "    #iterate over all three tasks\n",
    "    for task_idx, task_description in [\n",
    "        (0, '(\"Both TAL & GATA\")'),\n",
    "        (1, '(\"contains GATA\")'),\n",
    "        (2, '(\"contains TAL\")')]:\n",
    "\n",
    "        fig = plt.figure(figsize=(20,2.5))\n",
    "\n",
    "        for method_num, (method, name) in enumerate(methods_and_names):\n",
    "            #In the subplot of dimensions 1xlen(methods_and_names), select\n",
    "            #the method_num+1 subplot to work with (is 1-indexed so works out)\n",
    "            ax = plt.subplot(1,len(methods_and_names),method_num+1)\n",
    "            \n",
    "            scatter_plot_x = [] #x-coordinates for scatter plot\n",
    "            scatter_plot_y = [] #y-coordinates for scatter plot\n",
    "            colors = [] #colors of the points\n",
    "            \n",
    "            #iterate over possible label combinations for all three tasks\n",
    "            #(different label combinations get colored differently)\n",
    "            #(0,0,0) = no motifs = black\n",
    "            #(0,1,0) = only GATA inserted = blue\n",
    "            #(0,0,1) = only TAL inserted = green\n",
    "            #(1,1,1) = both TAL and GATA inserted = red\n",
    "            for labels,alpha,color,legend_label in [[(0,0,0), 0.3, (0,0,0), 'empty'],\n",
    "                                                    [(0,1,0), 0.3, (0,0,1), 'gata only'],\n",
    "                                                    [(0,0,1), 0.3, (0,0.7,0), 'tal only'],\n",
    "                                                    [(1,1,1), 0.3, (1,0,0), 'gata & tal']]:\n",
    "                #indices_to_keep_mask hones in on only those regions that\n",
    "                #satisfy the label combination in question\n",
    "                indices_to_keep_mask = [\n",
    "                    True if (x[0]==labels[0] and x[1]==labels[1] and x[2]==labels[2])\n",
    "                    else False for x in data.labels]\n",
    "                #get the motif scores in only the regions that satisfy the\n",
    "                #label combination of interest\n",
    "                log_odds_scores = np.compress(\n",
    "                    condition=indices_to_keep_mask,\n",
    "                    a=motif_to_top_N_log_odds_scores[motif_name],\n",
    "                    axis=0)\n",
    "                #as well as the importance scores on only the regions that satisfy the\n",
    "                #label combination of interest\n",
    "                sum_scores = np.compress(\n",
    "                    condition=indices_to_keep_mask,\n",
    "                    a=motif_to_task_to_method_to_corresponding_score[motif_name][task_idx][method],\n",
    "                    axis=0)\n",
    "                \n",
    "                #compile the motif log-odds scores and the corresponding importance scores into\n",
    "                #a two-dimensional array for all cases where the motif log-odds score is > 0\n",
    "                #(the cases where the motif log-odds score is < 0 can get very noisy because strong positive\n",
    "                # matches to a particular motif register as very strong negative matches to\n",
    "                # other motifs and can thus still have high importance scores)\n",
    "                scatter_plot_coords =\\\n",
    "                 np.array([x for x in zip(log_odds_scores.ravel(), sum_scores.ravel()) if x[0] > 0])\n",
    "                #extract the x and y indices into their separate arrays, as we will need to shuffle\n",
    "                #this so that datapoints of one color don't completely dominate other colors\n",
    "                scatter_plot_x.extend(scatter_plot_coords[:,0])\n",
    "                scatter_plot_y.extend(scatter_plot_coords[:,1])\n",
    "                colors.extend([color for i in range(len(scatter_plot_coords))])\n",
    "\n",
    "                #also keep track of the fraction of cases where both TAL and GATA were present in the\n",
    "                #sequence but a strong match to the motif got < 0 importance (false negatives)\n",
    "                if (task_idx == 0 and labels==(1,1,1)):\n",
    "                    threshold = 7\n",
    "                    red_zeros_picked_out= [(x, y) for x,y in\n",
    "                                            zip(scatter_plot_coords[:,0], scatter_plot_coords[:,1])\n",
    "                                            if (y <= 0) and (x >= threshold)]\n",
    "                    frac_task_0_red_le_zero[name] = float(len(red_zeros_picked_out))/len([x for x in scatter_plot_coords[:,0] if x >= threshold])\n",
    "\n",
    "            #shuffle the points so that datapoints of one color don't completely\n",
    "            #dominate other colors\n",
    "            shuffle_arrays([scatter_plot_x, scatter_plot_y, colors])\n",
    "            #make the plot\n",
    "            the_plot = ax.scatter(scatter_plot_x, scatter_plot_y,\n",
    "                                  alpha=alpha, color=colors)\n",
    "            \n",
    "            #add in labels where appropriate (the if statements are so that the labels don't\n",
    "            #get added to every single plot, but only at the edges)\n",
    "            if (method_num==0):\n",
    "                ax.set_ylabel(\"Scores for Task \"+str(task_idx)+\"\\n\"+task_description, fontsize=15)\n",
    "            if (method_num==int(len(methods_and_names)/2) and task_idx==2):\n",
    "                ax.set_xlabel(motif_name+\" log odds score\", fontsize=20)\n",
    "            if (task_idx!=2):\n",
    "                ax.set_xticks([])\n",
    "            ax.tick_params(axis='both', which='major', labelsize=13)\n",
    "            if (task_idx==0):\n",
    "                ax.set_title(name, fontsize=20)\n",
    "\n",
    "            #Setting the y-axes\n",
    "            #The thing to note here is that we use the same scale as task 0 for the \"off\" task\n",
    "            #the 'off' task is task 1 for TAL or task 2 for GATA. This makes it easier to\n",
    "            #judge if the method has a tendency to produce false positives.\n",
    "            max_y = np.max(np.abs(scatter_plot_y))\n",
    "            if (task_idx == 0):\n",
    "                method_to_task_0_max_y[method] = max_y\n",
    "            if ((task_idx==1 and motif_name==\"TAL1_known1\") or (task_idx==2 and motif_name==\"GATA_disc1\")):\n",
    "                max_y = method_to_task_0_max_y[method]            \n",
    "            \n",
    "            if ((task_idx==1 and motif_name==\"GATA_disc1\") or (task_idx==2 and motif_name==\"TAL1_known1\")):\n",
    "                ax.set_ylim((-0.1*max_y, 1.1*max_y))\n",
    "            else:\n",
    "                ax.set_ylim((-0.4*max_y, 1.1*max_y))\n",
    "                \n",
    "            ax.set_xlim(0, np.max(scatter_plot_x)*1.1)\n",
    "            ax.plot([0,np.max(scatter_plot_x)], [0,0], color=\"black\")\n",
    "\n",
    "    #plot stuff!\n",
    "    plt.show()\n",
    "    \n",
    "    #also make a barplot of the fraction of \"false negatives\"\n",
    "    fig,ax = plt.subplots(figsize=(20,2.5))\n",
    "    ind = np.arange(len(frac_task_0_red_le_zero.values()))\n",
    "    width=0.35\n",
    "    heights = np.array(frac_task_0_red_le_zero.values())\n",
    "    rects = ax.bar(ind, heights, width, color='red')\n",
    "    ax.set_xticks(ind + 0.5*width)\n",
    "    ax.set_xticklabels(frac_task_0_red_le_zero.keys(),fontsize=14)\n",
    "    ax.set_ylim(0, max(heights)*1.4)\n",
    "    plt.tick_params(labelsize=18)\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2.,\n",
    "                1.05*height, str(round(100*height,2))+\"%\",\n",
    "                ha='center', va='bottom', fontsize=18)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
